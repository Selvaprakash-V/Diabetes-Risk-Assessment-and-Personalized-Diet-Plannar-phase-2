{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f358d7",
   "metadata": {},
   "source": [
    "# Train RandomForest on Diabetes dataset\n",
    "\n",
    "This notebook loads the diabetes dataset in `ml/data/diabetes.csv`, performs cleaning and preprocessing, trains a RandomForest classifier, evaluates it, and saves a trained pipeline (model + scaler + metadata) to `backend/model/model.pkl`.\n",
    "\n",
    "Run each cell in order. The notebook is intended to be runnable in the project's Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and setup\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Configure paths\n",
    "BASE_DIR = Path('c:/gdg/diabetes-diet-planner') if os.name == 'nt' else Path('.')\n",
    "DATA_PATH = BASE_DIR / 'ml' / 'data' / 'diabetes.csv'\n",
    "MODEL_DIR = BASE_DIR / 'backend' / 'model'\n",
    "MODEL_FILE = MODEL_DIR / 'model.pkl'\n",
    "logger.info(f'DATA_PATH: {DATA_PATH}')\n",
    "logger.info(f'MODEL_FILE: {MODEL_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Load dataset\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f'Could not find dataset at {DATA_PATH}. Make sure the file exists.')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "logger.info(f'Dataset loaded: shape={df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Quick EDA\n",
    "display(df.describe(include='all'))\n",
    "print('\n",
    "Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Identify common Pima columns that sometimes contain zeros treated as missing\n",
    "possible_missing_zero_cols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "present_zero_counts = {}\n",
    "for c in possible_missing_zero_cols:\n",
    "    if c in df.columns:\n",
    "        present_zero_counts[c] = int((df[c] == 0).sum())\n",
    "\n",
    "print('\n",
    "Zero counts (these may indicate missing values):')\n",
    "print(present_zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Data cleaning - replace zeros with NaN and impute\n",
    "df_clean = df.copy()\n",
    "cols_to_fix = [c for c in ['Glucose','BloodPressure','SkinThickness','Insulin','BMI'] if c in df_clean.columns]\n",
    "for c in cols_to_fix:\n",
    "    n_zeros = int((df_clean[c] == 0).sum())\n",
    "    if n_zeros > 0:\n",
    "        logger.info(f'Replacing {n_zeros} zeros in {c} with NaN and imputing median')\n",
    "        df_clean[c].replace(0, np.nan, inplace=True)\n",
    "        median_val = df_clean[c].median()\n",
    "        df_clean[c].fillna(median_val, inplace=True)\n",
    "\n",
    "# Drop duplicates if any\n",
    "n_before = len(df_clean)\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "n_after = len(df_clean)\n",
    "logger.info(f'Dropped {n_before - n_after} duplicate rows')\n",
    "\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Feature / target preparation\n",
    "# Detect target column (common Pima column is 'Outcome')\n",
    "possible_target_names = ['Outcome','outcome','target','Target']\n",
    "target_col = None\n",
    "for t in possible_target_names:\n",
    "    if t in df_clean.columns:\n",
    "        target_col = t\n",
    "        break\n",
    "if target_col is None:\n",
    "    # fallback: assume last column is target\n",
    "    target_col = df_clean.columns[-1]\n",
    "    logger.warning(f'No common target column found. Falling back to last column: {target_col}')\n",
    "\n",
    "# Define features\n",
    "default_features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "features = [f for f in default_features if f in df_clean.columns]\n",
    "if not features:\n",
    "    # if none of the expected features exist, use all except target\n",
    "    features = [c for c in df_clean.columns if c != target_col]\n",
    "\n",
    "logger.info(f'Using target: {target_col} and features: {features}')\n",
    "X = df_clean[features].copy()\n",
    "y = df_clean[target_col].copy()\n",
    "\n",
    "# Show class distribution\n",
    "print('Target distribution:')\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ef00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Train/test split and scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "logger.info(f'Train shape: {X_train.shape}, Test shape: {X_test.shape}')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76386386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Train RandomForest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "t0 = time.time()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "train_time = time.time() - t0\n",
    "logger.info(f'Training completed in {train_time:.2f} seconds')\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "logger.info(f'Test accuracy: {acc:.4f}')\n",
    "print('\n",
    "Classification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix:\n",
    "', cm)\n",
    "\n",
    "# ROC AUC if possible\n",
    "if hasattr(clf, 'predict_proba'):\n",
    "    y_proba = clf.predict_proba(X_test_scaled)[:,1]\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        logger.info(f'ROC AUC: {auc:.4f}')\n",
    "    except Exception as e:\n",
    "        logger.warning(f'ROC AUC not computed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a13752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Save model and artifacts to backend/model\n",
    "MODEL_DIR = Path(MODEL_DIR)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "artifact = {'model': clf, 'scaler': scaler, 'features': features, 'target_col': target_col, 'trained_at': time.time(), 'random_state': RANDOM_STATE}\n",
    "with open(MODEL_FILE, 'wb') as f:\n",
    "    pickle.dump(artifact, f)\n",
    "logger.info(f'Model saved to: {MODEL_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b01a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Quick sanity checks\n",
    "assert MODEL_FILE.exists(), 'Model file was not created'\n",
    "print('Model file exists:', MODEL_FILE)\n",
    "# sample prediction\n",
    "sample_X = X_test.iloc[:3]\n",
    "sample_scaled = scaler.transform(sample_X)\n",
    "sample_pred = clf.predict(sample_scaled)\n",
    "print('Sample predictions (first 3 test rows):', sample_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608538c",
   "metadata": {},
   "source": [
    "## Notes & Next steps\n",
    "- Consider using a scikit-learn Pipeline to combine preprocessing and estimator for cleaner serialization.\n",
    "- Add hyperparameter tuning (GridSearchCV / RandomizedSearchCV) and persist CV results.\n",
    "- Save model metadata (metrics, feature importances) alongside the artifact.\n",
    "- If model file is large, prefer saving to an artifact store rather than committing to Git."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
